# ğŸ¤– Sistema RAG - IngestÃ£o e Busca SemÃ¢ntica com LangChain

![GitHub release](https://img.shields.io/github/v/release/Berchon/mba-ia-desafio-ingestao-busca)

Sistema de RecuperaÃ§Ã£o e GeraÃ§Ã£o Aumentada (RAG) que permite fazer perguntas sobre documentos PDF usando busca semÃ¢ntica e LLMs.

## ğŸ“‹ Ãndice

- [Sobre o Projeto](#-sobre-o-projeto)
- [Tecnologias Utilizadas](#-tecnologias-utilizadas)
- [PrÃ©-requisitos](#-prÃ©-requisitos)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [Uso](#-uso)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Como Funciona](#-como-funciona)
- [Comandos DisponÃ­veis](#-comandos-disponÃ­veis)
- [Troubleshooting](#-troubleshooting)

## ğŸ¯ Sobre o Projeto

Este sistema permite:

1. **IngestÃ£o de PDFs**: Carrega documentos PDF, divide em chunks e armazena embeddings em banco vetorial
2. **Busca SemÃ¢ntica**: Realiza buscas semÃ¢nticas usando similaridade de vetores
3. **Respostas Contextualizadas**: Usa LLMs para gerar respostas baseadas apenas no conteÃºdo dos documentos
4. **Interface CLI**: InteraÃ§Ã£o via linha de comando com comandos especiais

### CaracterÃ­sticas Principais

- âœ… Barra de progresso visual durante a ingestÃ£o (`tqdm`)
- âœ… Sistema de IDs determinÃ­sticos baseados em arquivo
- âœ… ConfirmaÃ§Ã£o de seguranÃ§a antes de sobrescrever documentos
- âœ… ExibiÃ§Ã£o de estatÃ­sticas detalhadas pÃ³s-ingestÃ£o
- âœ… Amostragem de fontes (arquivo e pÃ¡gina) nas respostas da IA
- âœ… Interface CLI interativa com comandos especiais (`add`, `clear`, `help`)
- âœ… Suporte completo a Google Gemini e OpenAI com abstraÃ§Ã£o de provedor
- âœ… Banco de dados vetorial PostgreSQL com pgVector via Repository Pattern

## ğŸ›  Tecnologias Utilizadas

### Core
- **Python 3.x**: Linguagem principal
- **LangChain**: Framework para aplicaÃ§Ãµes com LLMs
- **PostgreSQL + pgVector**: Banco de dados vetorial

### Bibliotecas Principais
- `langchain-google-genai`: IntegraÃ§Ã£o com Google Gemini
- `langchain-openai`: IntegraÃ§Ã£o com OpenAI
- `langchain-postgres`: IntegraÃ§Ã£o com PGVector
- `pypdf`: Leitura de arquivos PDF
- `python-dotenv`: Gerenciamento de variÃ¡veis de ambiente
- `psycopg`: Driver PostgreSQL

### Infraestrutura
- **Docker & Docker Compose**: ContainerizaÃ§Ã£o do banco de dados
- **pgVector**: ExtensÃ£o PostgreSQL para busca vetorial

## ğŸ“¦ PrÃ©-requisitos

- Python 3.10 ou superior
- Docker e Docker Compose
- Chave de API do Google Gemini OU OpenAI

### Obter Chaves de API

**Google Gemini:**
1. Acesse [Google AI Studio](https://aistudio.google.com/app/apikey)
2. Crie uma nova API Key
3. Copie a chave gerada

**OpenAI (opcional):**
1. Acesse [OpenAI Platform](https://platform.openai.com/api-keys)
2. Crie uma nova API Key
3. Copie a chave gerada

## ğŸš€ InstalaÃ§Ã£o

### 1. Clone o RepositÃ³rio

```bash
git clone <url-do-repositorio>
cd mba-ia-desafio-ingestao-busca
```

### 2. Crie o Ambiente Virtual

```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

### 3. Instale as DependÃªncias

```bash
pip install -r requirements.txt
```

### 4. Suba o Banco de Dados

O projeto usa Docker Compose para gerenciar o PostgreSQL com pgVector:

```bash
docker compose up -d
```

**O que este comando faz:**
- Cria um container PostgreSQL com a extensÃ£o pgVector habilitada
- ExpÃµe a porta 5432 para conexÃµes
- Cria um volume persistente para os dados
- Configura health checks automÃ¡ticos
- Inicializa a extensÃ£o vector automaticamente

**Verificar se estÃ¡ rodando:**
```bash
docker compose ps
```

**Parar o banco:**
```bash
docker compose down
```

**Parar e remover dados:**
```bash
docker compose down -v
```

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. Crie o Arquivo `.env`

Copie o arquivo de exemplo:

```bash
cp .env.example .env
```

### 2. Configure as VariÃ¡veis de Ambiente

Edite o arquivo `.env` com suas configuraÃ§Ãµes:

```bash
# === API Keys (configure pelo menos uma) ===

# Google Gemini (recomendado)
GOOGLE_API_KEY=sua_chave_google_aqui
GOOGLE_EMBEDDING_MODEL='models/embedding-001'
GOOGLE_LLM_MODEL='gemini-2.5-flash-lite'

# OpenAI (opcional)
OPENAI_API_KEY=sua_chave_openai_aqui
OPENAI_EMBEDDING_MODEL='text-embedding-3-small'
OPENAI_LLM_MODEL='gpt-4o-mini'

# === ConfiguraÃ§Ã£o do Banco de Dados ===
DATABASE_URL='postgresql://postgres:postgres@localhost:5432/rag'
PG_VECTOR_COLLECTION_NAME='pdf_embeddings'

# === ConfiguraÃ§Ã£o de Documentos ===
PDF_PATH=document.pdf
```

### DescriÃ§Ã£o das VariÃ¡veis

#### API Keys
- **GOOGLE_API_KEY**: Chave de API do Google Gemini (obtenha em https://aistudio.google.com)
- **GOOGLE_EMBEDDING_MODEL**: Modelo de embeddings do Google (padrÃ£o: `models/embedding-001`)
- **GOOGLE_LLM_MODEL**: Modelo de LLM do Google (padrÃ£o: `gemini-2.5-flash-lite`)
- **OPENAI_API_KEY**: Chave de API da OpenAI (opcional, obtenha em https://platform.openai.com)
- **OPENAI_EMBEDDING_MODEL**: Modelo de embeddings da OpenAI (padrÃ£o: `text-embedding-3-small`)
- **OPENAI_LLM_MODEL**: Modelo de LLM da OpenAI (padrÃ£o: `gpt-4o-mini`)

> **Nota**: O sistema detecta automaticamente qual provedor usar baseado nas chaves configuradas. Se ambas estiverem configuradas, o Google Gemini terÃ¡ prioridade.

#### Banco de Dados
- **DATABASE_URL**: URL de conexÃ£o com PostgreSQL
  - Formato: `postgresql://usuario:senha@host:porta/database`
  - Para desenvolvimento local com Docker Compose: `postgresql://postgres:postgres@localhost:5432/rag`
  - **Nota de SeguranÃ§a**: As credenciais `postgres:postgres` sÃ£o as padrÃ£o do `docker-compose.yml` fornecido. Para ambientes de produÃ§Ã£o, altere usuÃ¡rio e senha tanto no `docker-compose.yml` quanto no `.env`
- **PG_VECTOR_COLLECTION_NAME**: Nome da coleÃ§Ã£o/tabela no banco vetorial (padrÃ£o: `pdf_embeddings`)

#### Documentos
- **PDF_PATH**: Caminho para o arquivo PDF padrÃ£o a ser ingerido (padrÃ£o: `document.pdf`)

## ğŸ’» Uso

### Fluxo Completo de Uso

#### 1. Ingerir um Documento PDF

**Importante**: Certifique-se de que a variÃ¡vel `PDF_PATH` estÃ¡ configurada no arquivo `.env` apontando para o PDF que deseja ingerir.

```bash
python src/ingest.py
```

**O que acontece:**
- Carrega o PDF especificado em `PDF_PATH` (ou `document.pdf` por padrÃ£o)
- Divide o texto em chunks de 1000 caracteres com overlap de 150
- Gera embeddings para cada chunk
- Armazena os vetores no PostgreSQL com pgVector
- Exibe progresso e estatÃ­sticas

**Exemplo de saÃ­da:**
```
INFO - Iniciando ingestÃ£o do PDF: document.pdf
INFO - PDF carregado: 34 pÃ¡ginas
INFO - Texto dividido em 67 chunks
INFO - Gerando embeddings e armazenando no banco de dados...
INFO - âœ“ IngestÃ£o concluÃ­da com sucesso!
INFO - Total de documentos no banco: 67
```

#### 2. Iniciar o Chat Interativo

```bash
python src/chat.py
```

**Exemplo de interaÃ§Ã£o:**
```
=== Sistema RAG - Chat Interativo ===
âœ“ Banco de dados conectado e populado
âœ“ Sistema pronto para responder perguntas

Digite 'help' para ver comandos disponÃ­veis ou 'sair' para encerrar.

FaÃ§a sua pergunta:
> Qual o faturamento da empresa SuperTechIABrazil?

ğŸ” Buscando informaÃ§Ãµes...
ğŸ’¡ Gerando resposta...

RESPOSTA:
O faturamento da empresa SuperTechIABrazil foi de 10 milhÃµes de reais.

FONTES:
- document.pdf (pÃ¡g 26)
- document.pdf (pÃ¡g 2)

---
FaÃ§a sua pergunta:
> sair

ğŸ‘‹ AtÃ© logo! Chat encerrado.
```

#### 3. Ingerir PDF via CLI

VocÃª tambÃ©m pode ingerir um PDF especÃ­fico diretamente pelo chat:

```bash
python src/chat.py -file caminho/para/documento.pdf
```

Ou durante o chat:
```
FaÃ§a sua pergunta:
> add novo_documento.pdf

ğŸ“„ Iniciando ingestÃ£o de: novo_documento.pdf
âœ“ IngestÃ£o concluÃ­da!
```

## ğŸ“ Estrutura do Projeto

```
mba-ia-desafio-ingestao-busca/
â”œâ”€â”€ .agent/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ development-workflow.md    # Workflow de desenvolvimento
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ chat.py                        # CLI de interaÃ§Ã£o
â”‚   â”œâ”€â”€ config.py                      # ConfiguraÃ§Ã£o centralizada
â”‚   â”œâ”€â”€ database.py                    # ConexÃ£o com PGVector
â”‚   â”œâ”€â”€ embeddings_manager.py          # Singleton Manager de Embeddings
â”‚   â”œâ”€â”€ ingest.py                      # Script de ingestÃ£o
â”‚   â”œâ”€â”€ llm_manager.py                 # Singleton Manager de LLM
â”‚   â”œâ”€â”€ logger.py                      # Sistema de logging centralizado
â”‚   â””â”€â”€ search.py                      # MÃ³dulo de busca semÃ¢ntica
â”œâ”€â”€ .env                               # VariÃ¡veis de ambiente (nÃ£o versionado)
â”œâ”€â”€ .env.example                       # Template de configuraÃ§Ã£o
â”œâ”€â”€ .gitignore                         # Arquivos ignorados pelo Git
â”œâ”€â”€ CHANGELOG.md                       # HistÃ³rico de mudanÃ§as
â”œâ”€â”€ docker-compose.yml                 # ConfiguraÃ§Ã£o do PostgreSQL
â”œâ”€â”€ document.pdf                       # PDF de exemplo
â”œâ”€â”€ README.md                          # Este arquivo
â”œâ”€â”€ requirements.txt                   # DependÃªncias Python
â”œâ”€â”€ requisitos.md                      # Requisitos do projeto
â””â”€â”€ TODOs.md                           # Checklist de melhorias
```

## ğŸ” Como Funciona

### 1. IngestÃ£o (ingest.py)

```
PDF â†’ Carregamento â†’ Chunking â†’ Embeddings â†’ PGVector
```

1. **Carregamento**: `PyPDFLoader` extrai texto do PDF
2. **Chunking**: `RecursiveCharacterTextSplitter` divide em chunks de 1000 caracteres (overlap 150)
3. **Embeddings**: Modelo de embeddings converte texto em vetores
4. **Armazenamento**: Vetores salvos no PostgreSQL com pgVector

### 2. Busca (search.py)

```
Pergunta â†’ Embedding â†’ Busca Vetorial â†’ Top 10 â†’ Contexto â†’ LLM â†’ Resposta
```

1. **VetorizaÃ§Ã£o**: Pergunta convertida em embedding
2. **Busca**: Similarity search retorna 10 chunks mais relevantes (k=10)
3. **Contexto**: Chunks concatenados formam o contexto
4. **Prompt**: Template com contexto + regras + pergunta
5. **LLM**: Modelo gera resposta baseada apenas no contexto

### 3. Prompt Template

O sistema usa um prompt rigoroso para evitar alucinaÃ§Ãµes:

```
CONTEXTO:
{chunks recuperados do banco}

REGRAS:
- Responda somente com base no CONTEXTO
- Se a informaÃ§Ã£o nÃ£o estiver no CONTEXTO, responda:
  "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."
- Nunca invente ou use conhecimento externo

PERGUNTA DO USUÃRIO:
{pergunta}
```

## ğŸ® Comandos DisponÃ­veis

No chat interativo, vocÃª pode usar:

| Comando | DescriÃ§Ã£o |
|---------|-----------|
| `help` | Exibe lista de comandos disponÃ­veis |
| `add <caminho>` ou `ingest <caminho>` | Ingere um novo PDF |
| `sair`, `exit`, `quit`, `q` | Encerra o chat |

## ğŸ› Troubleshooting

### Erro: "Database connection failed"

**Problema**: NÃ£o consegue conectar ao PostgreSQL

**SoluÃ§Ãµes**:
1. Verifique se o Docker estÃ¡ rodando: `docker compose ps`
2. Suba o banco: `docker compose up -d`
3. Verifique a `DATABASE_URL` no `.env`
4. Teste a conexÃ£o: `docker exec -it postgres_rag psql -U postgres -d rag`

### Erro: "API key not found"

**Problema**: Chave de API nÃ£o configurada

**SoluÃ§Ãµes**:
1. Verifique se o arquivo `.env` existe
2. Confirme que `GOOGLE_API_KEY` ou `OPENAI_API_KEY` estÃ¡ preenchida
3. NÃ£o use aspas ao redor da chave no `.env`

### Erro: "No documents in database"

**Problema**: Banco de dados vazio

**SoluÃ§Ãµes**:
1. Execute a ingestÃ£o: `python src/ingest.py`
2. Ou use o comando `add` no chat: `add document.pdf`

### Erro: "PDF not found"

**Problema**: Arquivo PDF nÃ£o encontrado

**SoluÃ§Ãµes**:
1. Verifique se o arquivo existe no caminho especificado
2. Use caminho absoluto ou relativo correto
3. Atualize `PDF_PATH` no `.env` se necessÃ¡rio

### Performance lenta

**Problema**: Respostas demoram muito

**SoluÃ§Ãµes**:
1. Use modelos mais rÃ¡pidos (ex: `gemini-2.5-flash-lite`)
2. Reduza o valor de `k` (nÃºmero de chunks recuperados)
3. Verifique sua conexÃ£o com a internet

## ğŸ“ PrÃ³ximos Passos

Consulte o arquivo [TODOs.md](TODOs.md) para ver as melhorias planejadas, incluindo:

- **Fase E: Melhorias TÃ©cnicas do Chat** (Argumentos CLI padrÃ£o, tratamento de banco vazio)
- **Fase F: Comandos Estendidos** (Comandos `stats` e `remove <arquivo>`)
- **Fase G: Melhorias de UX** (SimplificaÃ§Ã£o de prompt, atalhos de comando)
- **Fase K: RefatoraÃ§Ãµes AvanÃ§adas** (HistÃ³rico de conversas, cache de embeddings)

## ğŸ¤– Desenvolvido com Antigravity

Este projeto foi inteiramente desenvolvido utilizando o **Antigravity**, o assistente de IA da Google para desenvolvimento de cÃ³digo. A escolha de usar o Antigravity como ferramenta principal teve como objetivo:

- **Aprendizado PrÃ¡tico**: Explorar as capacidades de um agente de IA moderno no desenvolvimento de software completo
- **Produtividade**: Acelerar o desenvolvimento mantendo qualidade e boas prÃ¡ticas
- **ExperimentaÃ§Ã£o**: Testar os limites da colaboraÃ§Ã£o humano-IA em projetos reais
- **DocumentaÃ§Ã£o**: Criar um caso de uso real e bem documentado do uso de IA no desenvolvimento

Todo o cÃ³digo, desde a arquitetura inicial atÃ© a implementaÃ§Ã£o de features, refatoraÃ§Ãµes e esta documentaÃ§Ã£o, foi criado em colaboraÃ§Ã£o com o Antigravity. Este projeto serve como exemplo prÃ¡tico de como ferramentas de IA podem auxiliar no desenvolvimento de aplicaÃ§Ãµes complexas envolvendo LLMs, bancos vetoriais e processamento de documentos.

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido como parte do MBA em InteligÃªncia Artificial da Full Cycle.

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Siga o workflow definido em `.agent/workflows/development-workflow.md`.

---

**Desenvolvido com â¤ï¸ usando LangChain e Google Gemini**